{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217333b7",
   "metadata": {},
   "source": [
    "轉化PoH 類似Linkedlist : Sender|PoH Value|Receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7abc729",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHADOOP_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbin;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ====== SparkSession ======\u001b[39;00m\n\u001b[0;32m     16\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPoH LinkedList & Edge Construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.maxResultSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetCheckpointDir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///C:/Users/Leon/Desktop/spark-checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# ====== 開始計時 ======\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, concat_ws, udf, lag, lead\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# ====== Java/Hadoop 路徑設定 ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== SparkSession ======\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PoH LinkedList & Edge Construction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "\n",
    "# ====== 開始計時 ======\n",
    "start_time = time.time()\n",
    "\n",
    "# ====== 讀取資料 ======\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\程式語言資料\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "out_dir = r\"C:\\Users\\Leon\\Desktop\\程式語言資料\\python\\TD-UF\\圖結構\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "\n",
    "# ====== 轉成 timestamp & 排序 ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(StringType()), col(\"Time\").cast(StringType()))))\n",
    "df = df.orderBy(\"timestamp\")\n",
    "\n",
    "# ====== row_idx 產生 PoH LinkedList ======\n",
    "w = Window.orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"row_idx\", row_number().over(w) - 1)\n",
    "\n",
    "# ====== 收集必要欄位成 Pandas DataFrame，計算 PoH 串鍊 ======\n",
    "df_pd = df.select(\"row_idx\", \"Sender_account\", \"Receiver_account\", \"tx_id\", \"timestamp\").orderBy(\"row_idx\").toPandas()\n",
    "\n",
    "pohs = []\n",
    "prev_poh = \"\"\n",
    "for _, row in df_pd.iterrows():\n",
    "    sender = str(row['Sender_account'])\n",
    "    receiver = str(row['Receiver_account'])\n",
    "    txid = str(row['tx_id'])\n",
    "    poh_input = prev_poh + sender + receiver + txid\n",
    "    poh = hashlib.sha256(poh_input.encode()).hexdigest()\n",
    "    pohs.append(poh)\n",
    "    prev_poh = poh\n",
    "\n",
    "df_pd[\"PoH_Value\"] = pohs\n",
    "\n",
    "# ====== 生成 Spark 節點表 DataFrame ======\n",
    "node_pd = df_pd[[\"Sender_account\", \"PoH_Value\", \"Receiver_account\", \"timestamp\"]]\n",
    "nodes_path = os.path.join(out_dir, \"nodes_full.csv\")\n",
    "node_pd.to_csv(nodes_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✔️ 節點表輸出完成：{nodes_path}\")\n",
    "\n",
    "# ====== 建 Receiver→Sender 有向邊表（找每個帳戶收到錢→首次轉出） ======\n",
    "# 收到錢的所有紀錄\n",
    "receiver_rows = {}\n",
    "for idx, row in df_pd.iterrows():\n",
    "    receiver = row['Receiver_account']\n",
    "    receiver_rows.setdefault(receiver, []).append(idx)\n",
    "\n",
    "# 首次出現當Sender的紀錄\n",
    "sender_first_rows = {}\n",
    "for idx, row in df_pd.iterrows():\n",
    "    sender = row['Sender_account']\n",
    "    if sender not in sender_first_rows:\n",
    "        sender_first_rows[sender] = idx\n",
    "\n",
    "edges = []\n",
    "for acct, recv_idxs in receiver_rows.items():\n",
    "    if acct in sender_first_rows:\n",
    "        send_idx = sender_first_rows[acct]\n",
    "        # 若第一次出現不是同一筆，才建邊\n",
    "        for recv_idx in recv_idxs:\n",
    "            if recv_idx < send_idx:\n",
    "                src = df_pd.loc[recv_idx, 'PoH_Value']\n",
    "                tgt = df_pd.loc[send_idx, 'PoH_Value']\n",
    "                edges.append([src, tgt, acct])\n",
    "\n",
    "edges_path = os.path.join(out_dir, \"edges_full.csv\")\n",
    "import pandas as pd\n",
    "edges_df = pd.DataFrame(edges, columns=['from_PoH', 'to_PoH', 'account'])\n",
    "edges_df.to_csv(edges_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✔️ 有向邊表輸出完成：{edges_path}\")\n",
    "\n",
    "# ====== 統計運算時間 ======\n",
    "end_time = time.time()\n",
    "print(f\"✨ 完成所有金流圖構建，總花費時間：{end_time - start_time:.2f} 秒\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c27dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動 SparkSession...\n",
      "✔️ SparkSession 建立完成\n",
      "📥 讀取資料中 ...\n",
      "✔️ 載入資料完成，共 9504852 筆交易\n",
      "🔄 產生 PoH 鏈式雜湊欄位 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 產生 PoH 雜湊鏈中...: 100%|██████████| 9504852/9504852 [00:11<00:00, 793376.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ PoH 雜湊計算完成\n",
      "🧩 建立交易圖（每筆交易為一個節點）...\n",
      "✔️ 交易節點數: 9504852，交易邊數: 0\n",
      "🔗 TD-UF 分群（每筆交易都有 component id）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 完成分群計算\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[1;32m---> 97\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_baseline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     df_baseline \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df_baseline)\n\u001b[0;32m    100\u001b[0m feature_cols_baseline \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ====== Java/Hadoop 路徑設定 ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== 建立 SparkSession ======\n",
    "print(\"🚀 啟動 SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF vs Baseline, PySpark LR, Sliding Window 5-Fold\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"✔️ SparkSession 建立完成\")\n",
    "\n",
    "# ====== 日期字串轉 epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== 讀取資料 ======\n",
    "print(\"📥 讀取資料中 ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\程式語言資料\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"✔️ 載入資料完成，共 {df.count()} 筆交易\")\n",
    "\n",
    "# ====== PoH 鏈式雜湊產生 ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"🔗 產生 PoH 雜湊鏈中...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"🔄 產生 PoH 鏈式雜湊欄位 ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"✔️ PoH 雜湊計算完成\")\n",
    "\n",
    "# ====== 交易圖建構：每筆交易為節點 ======\n",
    "print(\"🧩 建立交易圖（每筆交易為一個節點）...\")\n",
    "edges_df = df.alias(\"a\").join(\n",
    "    df.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"a.tx_id\").alias(\"src\"),\n",
    "    col(\"b.tx_id\").alias(\"dst\")\n",
    ").distinct()\n",
    "vertices = df.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "print(f\"✔️ 交易節點數: {vertices.count()}，交易邊數: {edges_df.count()}\")\n",
    "\n",
    "print(\"🔗 TD-UF 分群（每筆交易都有 component id）...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"✔️ 完成分群計算\")\n",
    "df = df.join(components, df.tx_id == components.id, \"left\").drop(\"id\")\n",
    "df = df.withColumn(\"component\", col(\"component\").cast(\"double\"))\n",
    "\n",
    "# ====== Baseline 特徵工程（含編碼）======\n",
    "df_baseline = df\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "for c in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df_baseline)\n",
    "    df_baseline = indexer.transform(df_baseline)\n",
    "\n",
    "feature_cols_baseline = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "feature_cols_tduf = [\"Amount\", \"timestamp\", \"component\"]\n",
    "\n",
    "# ========== 加入時間排序 row_id ==========\n",
    "window = Window.orderBy(\"timestamp\")\n",
    "df_baseline = df_baseline.withColumn(\"row_id\", row_number().over(window) - 1)\n",
    "df = df_baseline  # baseline和tduf保證排序相同\n",
    "total_count = df.count()\n",
    "K = 5\n",
    "fold_size = total_count // (K+1)   # 注意sliding window預留測試區\n",
    "\n",
    "# ====== Sliding window 設計：每次擴展訓練集，測試集為下一區間 ======\n",
    "def sliding_window_splits(total_count, fold_size, K):\n",
    "    splits = []\n",
    "    for k in range(K):\n",
    "        train_end = (k+1) * fold_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + fold_size\n",
    "        if test_end > total_count:\n",
    "            test_end = total_count\n",
    "        splits.append((0, train_end, test_start, test_end))\n",
    "    return splits\n",
    "\n",
    "splits = sliding_window_splits(total_count, fold_size, K)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Is_laundering\", predictionCol=\"prediction\")\n",
    "\n",
    "# ========== TD-UF+LR sliding window 5-fold ==========\n",
    "print(\"\\n==== ⏳ TD-UF+LR 滑動視窗五折 ====\")\n",
    "metrics_tduf, all_true_tduf, all_pred_tduf, all_proba_tduf = [], [], [], []\n",
    "for i, (train_start, train_end, test_start, test_end) in enumerate(splits):\n",
    "    train_df = df.filter((col(\"row_id\") >= train_start) & (col(\"row_id\") < train_end))\n",
    "    test_df  = df.filter((col(\"row_id\") >= test_start) & (col(\"row_id\") < test_end))\n",
    "    n_train_pos = train_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    n_test_pos = test_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    print(f\"[TD-UF] Fold {i+1}: 訓練集({train_end-train_start})正樣本={n_train_pos}，測試集({test_end-test_start})正樣本={n_test_pos}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols_tduf, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"Is_laundering\")\n",
    "    test_data  = assembler.transform(test_df).select(\"features\", \"Is_laundering\")\n",
    "\n",
    "    lr = SparkLR(maxIter=100, regParam=0.01, labelCol=\"Is_laundering\", probabilityCol=\"probability\")\n",
    "    t1 = time.time()\n",
    "    model = lr.fit(train_data)\n",
    "    pred = model.transform(test_data)\n",
    "    t2 = time.time()\n",
    "\n",
    "    acc = evaluator.setMetricName(\"accuracy\").evaluate(pred)\n",
    "    prec = evaluator.setMetricName(\"weightedPrecision\").evaluate(pred)\n",
    "    rec = evaluator.setMetricName(\"weightedRecall\").evaluate(pred)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(pred)\n",
    "    metrics_tduf.append((acc, prec, rec, f1, t2-t1))\n",
    "\n",
    "    pred_pd = pred.select(\"Is_laundering\", \"prediction\", \"probability\").toPandas()\n",
    "    all_true_tduf.append(pred_pd[\"Is_laundering\"].values)\n",
    "    all_pred_tduf.append(pred_pd[\"prediction\"].values)\n",
    "    all_proba_tduf.append(pred_pd[\"probability\"].apply(lambda x: x[1] if hasattr(x, \"__getitem__\") else x).values)\n",
    "\n",
    "y_true_tduf = np.concatenate(all_true_tduf)\n",
    "y_pred_tduf = np.concatenate(all_pred_tduf)\n",
    "y_proba_tduf = np.concatenate(all_proba_tduf)\n",
    "metrics_tduf = np.array(metrics_tduf)\n",
    "\n",
    "print(\"\\n📌 TD-UF+LR（sliding window 五折驗證）\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(metrics_tduf[:,0])), \n",
    "    (\"Precision\", np.mean(metrics_tduf[:,1])),\n",
    "    (\"Recall\", np.mean(metrics_tduf[:,2])),\n",
    "    (\"F1 Score\", np.mean(metrics_tduf[:,3])),\n",
    "    (\"Avg Time\", np.mean(metrics_tduf[:,4]))\n",
    "], headers=[\"指標\", \"平均值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== 畫出 TD-UF+LR 的 ROC 曲線 ======\n",
    "fpr, tpr, _ = roc_curve(y_true_tduf, y_proba_tduf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'TD-UF+LR (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# ====== Baseline sliding window 5-fold（同一組分割）=====\n",
    "print(\"\\n==== ⏳ Baseline LR sliding window 五折 ====\")\n",
    "metrics_base, all_true_base, all_pred_base, all_proba_base = [], [], [], []\n",
    "for i, (train_start, train_end, test_start, test_end) in enumerate(splits):\n",
    "    train_df = df.filter((col(\"row_id\") >= train_start) & (col(\"row_id\") < train_end))\n",
    "    test_df  = df.filter((col(\"row_id\") >= test_start) & (col(\"row_id\") < test_end))\n",
    "    n_train_pos = train_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    n_test_pos = test_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    print(f\"[Baseline] Fold {i+1}: 訓練集({train_end-train_start})正樣本={n_train_pos}，測試集({test_end-test_start})正樣本={n_test_pos}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols_baseline, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"Is_laundering\")\n",
    "    test_data  = assembler.transform(test_df).select(\"features\", \"Is_laundering\")\n",
    "\n",
    "    lr = SparkLR(maxIter=100, regParam=0.01, labelCol=\"Is_laundering\", probabilityCol=\"probability\")\n",
    "    t1 = time.time()\n",
    "    model = lr.fit(train_data)\n",
    "    pred = model.transform(test_data)\n",
    "    t2 = time.time()\n",
    "\n",
    "    acc = evaluator.setMetricName(\"accuracy\").evaluate(pred)\n",
    "    prec = evaluator.setMetricName(\"weightedPrecision\").evaluate(pred)\n",
    "    rec = evaluator.setMetricName(\"weightedRecall\").evaluate(pred)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(pred)\n",
    "    metrics_base.append((acc, prec, rec, f1, t2-t1))\n",
    "\n",
    "    pred_pd = pred.select(\"Is_laundering\", \"prediction\", \"probability\").toPandas()\n",
    "    all_true_base.append(pred_pd[\"Is_laundering\"].values)\n",
    "    all_pred_base.append(pred_pd[\"prediction\"].values)\n",
    "    all_proba_base.append(pred_pd[\"probability\"].apply(lambda x: x[1] if hasattr(x, \"__getitem__\") else x).values)\n",
    "\n",
    "y_true_base = np.concatenate(all_true_base)\n",
    "y_pred_base = np.concatenate(all_pred_base)\n",
    "y_proba_base = np.concatenate(all_proba_base)\n",
    "metrics_base = np.array(metrics_base)\n",
    "\n",
    "print(\"\\n📌 Baseline LR（sliding window 五折驗證）\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(metrics_base[:,0])), \n",
    "    (\"Precision\", np.mean(metrics_base[:,1])),\n",
    "    (\"Recall\", np.mean(metrics_base[:,2])),\n",
    "    (\"F1 Score\", np.mean(metrics_base[:,3])),\n",
    "    (\"Avg Time\", np.mean(metrics_base[:,4]))\n",
    "], headers=[\"指標\", \"平均值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== 畫出 Baseline LR 的 ROC 曲線 ======\n",
    "fpr_b, tpr_b, _ = roc_curve(y_true_base, y_proba_base)\n",
    "roc_auc_b = auc(fpr_b, tpr_b)\n",
    "plt.plot(fpr_b, tpr_b, label=f'Baseline LR (AUC = {roc_auc_b:.4f})')\n",
    "\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: TD-UF+LR vs Baseline (Sliding Window 5-Fold)\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"both\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"✅ 全部流程執行完畢！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
