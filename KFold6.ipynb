{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217333b7",
   "metadata": {},
   "source": [
    "è½‰åŒ–PoH é¡ä¼¼Linkedlist : Sender|PoH Value|Receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7abc729",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHADOOP_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbin;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ====== SparkSession ======\u001b[39;00m\n\u001b[0;32m     16\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPoH LinkedList & Edge Construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.maxResultSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetCheckpointDir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///C:/Users/Leon/Desktop/spark-checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# ====== é–‹å§‹è¨ˆæ™‚ ======\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, concat_ws, udf, lag, lead\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# ====== Java/Hadoop è·¯å¾‘è¨­å®š ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== SparkSession ======\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PoH LinkedList & Edge Construction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "\n",
    "# ====== é–‹å§‹è¨ˆæ™‚ ======\n",
    "start_time = time.time()\n",
    "\n",
    "# ====== è®€å–è³‡æ–™ ======\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "out_dir = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\åœ–çµæ§‹\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "\n",
    "# ====== è½‰æˆ timestamp & æ’åº ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(StringType()), col(\"Time\").cast(StringType()))))\n",
    "df = df.orderBy(\"timestamp\")\n",
    "\n",
    "# ====== row_idx ç”¢ç”Ÿ PoH LinkedList ======\n",
    "w = Window.orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"row_idx\", row_number().over(w) - 1)\n",
    "\n",
    "# ====== æ”¶é›†å¿…è¦æ¬„ä½æˆ Pandas DataFrameï¼Œè¨ˆç®— PoH ä¸²éŠ ======\n",
    "df_pd = df.select(\"row_idx\", \"Sender_account\", \"Receiver_account\", \"tx_id\", \"timestamp\").orderBy(\"row_idx\").toPandas()\n",
    "\n",
    "pohs = []\n",
    "prev_poh = \"\"\n",
    "for _, row in df_pd.iterrows():\n",
    "    sender = str(row['Sender_account'])\n",
    "    receiver = str(row['Receiver_account'])\n",
    "    txid = str(row['tx_id'])\n",
    "    poh_input = prev_poh + sender + receiver + txid\n",
    "    poh = hashlib.sha256(poh_input.encode()).hexdigest()\n",
    "    pohs.append(poh)\n",
    "    prev_poh = poh\n",
    "\n",
    "df_pd[\"PoH_Value\"] = pohs\n",
    "\n",
    "# ====== ç”Ÿæˆ Spark ç¯€é»è¡¨ DataFrame ======\n",
    "node_pd = df_pd[[\"Sender_account\", \"PoH_Value\", \"Receiver_account\", \"timestamp\"]]\n",
    "nodes_path = os.path.join(out_dir, \"nodes_full.csv\")\n",
    "node_pd.to_csv(nodes_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ”ï¸ ç¯€é»è¡¨è¼¸å‡ºå®Œæˆï¼š{nodes_path}\")\n",
    "\n",
    "# ====== å»º Receiverâ†’Sender æœ‰å‘é‚Šè¡¨ï¼ˆæ‰¾æ¯å€‹å¸³æˆ¶æ”¶åˆ°éŒ¢â†’é¦–æ¬¡è½‰å‡ºï¼‰ ======\n",
    "# æ”¶åˆ°éŒ¢çš„æ‰€æœ‰ç´€éŒ„\n",
    "receiver_rows = {}\n",
    "for idx, row in df_pd.iterrows():\n",
    "    receiver = row['Receiver_account']\n",
    "    receiver_rows.setdefault(receiver, []).append(idx)\n",
    "\n",
    "# é¦–æ¬¡å‡ºç¾ç•¶Senderçš„ç´€éŒ„\n",
    "sender_first_rows = {}\n",
    "for idx, row in df_pd.iterrows():\n",
    "    sender = row['Sender_account']\n",
    "    if sender not in sender_first_rows:\n",
    "        sender_first_rows[sender] = idx\n",
    "\n",
    "edges = []\n",
    "for acct, recv_idxs in receiver_rows.items():\n",
    "    if acct in sender_first_rows:\n",
    "        send_idx = sender_first_rows[acct]\n",
    "        # è‹¥ç¬¬ä¸€æ¬¡å‡ºç¾ä¸æ˜¯åŒä¸€ç­†ï¼Œæ‰å»ºé‚Š\n",
    "        for recv_idx in recv_idxs:\n",
    "            if recv_idx < send_idx:\n",
    "                src = df_pd.loc[recv_idx, 'PoH_Value']\n",
    "                tgt = df_pd.loc[send_idx, 'PoH_Value']\n",
    "                edges.append([src, tgt, acct])\n",
    "\n",
    "edges_path = os.path.join(out_dir, \"edges_full.csv\")\n",
    "import pandas as pd\n",
    "edges_df = pd.DataFrame(edges, columns=['from_PoH', 'to_PoH', 'account'])\n",
    "edges_df.to_csv(edges_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ”ï¸ æœ‰å‘é‚Šè¡¨è¼¸å‡ºå®Œæˆï¼š{edges_path}\")\n",
    "\n",
    "# ====== çµ±è¨ˆé‹ç®—æ™‚é–“ ======\n",
    "end_time = time.time()\n",
    "print(f\"âœ¨ å®Œæˆæ‰€æœ‰é‡‘æµåœ–æ§‹å»ºï¼Œç¸½èŠ±è²»æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c27dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹• SparkSession...\n",
      "âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\n",
      "ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\n",
      "âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± 9504852 ç­†äº¤æ˜“\n",
      "ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9504852/9504852 [00:11<00:00, 793376.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\n",
      "ğŸ§© å»ºç«‹äº¤æ˜“åœ–ï¼ˆæ¯ç­†äº¤æ˜“ç‚ºä¸€å€‹ç¯€é»ï¼‰...\n",
      "âœ”ï¸ äº¤æ˜“ç¯€é»æ•¸: 9504852ï¼Œäº¤æ˜“é‚Šæ•¸: 0\n",
      "ğŸ”— TD-UF åˆ†ç¾¤ï¼ˆæ¯ç­†äº¤æ˜“éƒ½æœ‰ component idï¼‰...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ å®Œæˆåˆ†ç¾¤è¨ˆç®—\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[1;32m---> 97\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_baseline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     df_baseline \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df_baseline)\n\u001b[0;32m    100\u001b[0m feature_cols_baseline \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ====== Java/Hadoop è·¯å¾‘è¨­å®š ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== å»ºç«‹ SparkSession ======\n",
    "print(\"ğŸš€ å•Ÿå‹• SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF vs Baseline, PySpark LR, Sliding Window 5-Fold\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\")\n",
    "\n",
    "# ====== æ—¥æœŸå­—ä¸²è½‰ epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== è®€å–è³‡æ–™ ======\n",
    "print(\"ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± {df.count()} ç­†äº¤æ˜“\")\n",
    "\n",
    "# ====== PoH éˆå¼é›œæ¹Šç”¢ç”Ÿ ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"âœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\")\n",
    "\n",
    "# ====== äº¤æ˜“åœ–å»ºæ§‹ï¼šæ¯ç­†äº¤æ˜“ç‚ºç¯€é» ======\n",
    "print(\"ğŸ§© å»ºç«‹äº¤æ˜“åœ–ï¼ˆæ¯ç­†äº¤æ˜“ç‚ºä¸€å€‹ç¯€é»ï¼‰...\")\n",
    "edges_df = df.alias(\"a\").join(\n",
    "    df.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"a.tx_id\").alias(\"src\"),\n",
    "    col(\"b.tx_id\").alias(\"dst\")\n",
    ").distinct()\n",
    "vertices = df.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "print(f\"âœ”ï¸ äº¤æ˜“ç¯€é»æ•¸: {vertices.count()}ï¼Œäº¤æ˜“é‚Šæ•¸: {edges_df.count()}\")\n",
    "\n",
    "print(\"ğŸ”— TD-UF åˆ†ç¾¤ï¼ˆæ¯ç­†äº¤æ˜“éƒ½æœ‰ component idï¼‰...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"âœ”ï¸ å®Œæˆåˆ†ç¾¤è¨ˆç®—\")\n",
    "df = df.join(components, df.tx_id == components.id, \"left\").drop(\"id\")\n",
    "df = df.withColumn(\"component\", col(\"component\").cast(\"double\"))\n",
    "\n",
    "# ====== Baseline ç‰¹å¾µå·¥ç¨‹ï¼ˆå«ç·¨ç¢¼ï¼‰======\n",
    "df_baseline = df\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "for c in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df_baseline)\n",
    "    df_baseline = indexer.transform(df_baseline)\n",
    "\n",
    "feature_cols_baseline = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "feature_cols_tduf = [\"Amount\", \"timestamp\", \"component\"]\n",
    "\n",
    "# ========== åŠ å…¥æ™‚é–“æ’åº row_id ==========\n",
    "window = Window.orderBy(\"timestamp\")\n",
    "df_baseline = df_baseline.withColumn(\"row_id\", row_number().over(window) - 1)\n",
    "df = df_baseline  # baselineå’Œtdufä¿è­‰æ’åºç›¸åŒ\n",
    "total_count = df.count()\n",
    "K = 5\n",
    "fold_size = total_count // (K+1)   # æ³¨æ„sliding windowé ç•™æ¸¬è©¦å€\n",
    "\n",
    "# ====== Sliding window è¨­è¨ˆï¼šæ¯æ¬¡æ“´å±•è¨“ç·´é›†ï¼Œæ¸¬è©¦é›†ç‚ºä¸‹ä¸€å€é–“ ======\n",
    "def sliding_window_splits(total_count, fold_size, K):\n",
    "    splits = []\n",
    "    for k in range(K):\n",
    "        train_end = (k+1) * fold_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + fold_size\n",
    "        if test_end > total_count:\n",
    "            test_end = total_count\n",
    "        splits.append((0, train_end, test_start, test_end))\n",
    "    return splits\n",
    "\n",
    "splits = sliding_window_splits(total_count, fold_size, K)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Is_laundering\", predictionCol=\"prediction\")\n",
    "\n",
    "# ========== TD-UF+LR sliding window 5-fold ==========\n",
    "print(\"\\n==== â³ TD-UF+LR æ»‘å‹•è¦–çª—äº”æŠ˜ ====\")\n",
    "metrics_tduf, all_true_tduf, all_pred_tduf, all_proba_tduf = [], [], [], []\n",
    "for i, (train_start, train_end, test_start, test_end) in enumerate(splits):\n",
    "    train_df = df.filter((col(\"row_id\") >= train_start) & (col(\"row_id\") < train_end))\n",
    "    test_df  = df.filter((col(\"row_id\") >= test_start) & (col(\"row_id\") < test_end))\n",
    "    n_train_pos = train_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    n_test_pos = test_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    print(f\"[TD-UF] Fold {i+1}: è¨“ç·´é›†({train_end-train_start})æ­£æ¨£æœ¬={n_train_pos}ï¼Œæ¸¬è©¦é›†({test_end-test_start})æ­£æ¨£æœ¬={n_test_pos}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols_tduf, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"Is_laundering\")\n",
    "    test_data  = assembler.transform(test_df).select(\"features\", \"Is_laundering\")\n",
    "\n",
    "    lr = SparkLR(maxIter=100, regParam=0.01, labelCol=\"Is_laundering\", probabilityCol=\"probability\")\n",
    "    t1 = time.time()\n",
    "    model = lr.fit(train_data)\n",
    "    pred = model.transform(test_data)\n",
    "    t2 = time.time()\n",
    "\n",
    "    acc = evaluator.setMetricName(\"accuracy\").evaluate(pred)\n",
    "    prec = evaluator.setMetricName(\"weightedPrecision\").evaluate(pred)\n",
    "    rec = evaluator.setMetricName(\"weightedRecall\").evaluate(pred)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(pred)\n",
    "    metrics_tduf.append((acc, prec, rec, f1, t2-t1))\n",
    "\n",
    "    pred_pd = pred.select(\"Is_laundering\", \"prediction\", \"probability\").toPandas()\n",
    "    all_true_tduf.append(pred_pd[\"Is_laundering\"].values)\n",
    "    all_pred_tduf.append(pred_pd[\"prediction\"].values)\n",
    "    all_proba_tduf.append(pred_pd[\"probability\"].apply(lambda x: x[1] if hasattr(x, \"__getitem__\") else x).values)\n",
    "\n",
    "y_true_tduf = np.concatenate(all_true_tduf)\n",
    "y_pred_tduf = np.concatenate(all_pred_tduf)\n",
    "y_proba_tduf = np.concatenate(all_proba_tduf)\n",
    "metrics_tduf = np.array(metrics_tduf)\n",
    "\n",
    "print(\"\\nğŸ“Œ TD-UF+LRï¼ˆsliding window äº”æŠ˜é©—è­‰ï¼‰\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(metrics_tduf[:,0])), \n",
    "    (\"Precision\", np.mean(metrics_tduf[:,1])),\n",
    "    (\"Recall\", np.mean(metrics_tduf[:,2])),\n",
    "    (\"F1 Score\", np.mean(metrics_tduf[:,3])),\n",
    "    (\"Avg Time\", np.mean(metrics_tduf[:,4]))\n",
    "], headers=[\"æŒ‡æ¨™\", \"å¹³å‡å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== ç•«å‡º TD-UF+LR çš„ ROC æ›²ç·š ======\n",
    "fpr, tpr, _ = roc_curve(y_true_tduf, y_proba_tduf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'TD-UF+LR (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# ====== Baseline sliding window 5-foldï¼ˆåŒä¸€çµ„åˆ†å‰²ï¼‰=====\n",
    "print(\"\\n==== â³ Baseline LR sliding window äº”æŠ˜ ====\")\n",
    "metrics_base, all_true_base, all_pred_base, all_proba_base = [], [], [], []\n",
    "for i, (train_start, train_end, test_start, test_end) in enumerate(splits):\n",
    "    train_df = df.filter((col(\"row_id\") >= train_start) & (col(\"row_id\") < train_end))\n",
    "    test_df  = df.filter((col(\"row_id\") >= test_start) & (col(\"row_id\") < test_end))\n",
    "    n_train_pos = train_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    n_test_pos = test_df.filter(col(\"Is_laundering\") == 1).count()\n",
    "    print(f\"[Baseline] Fold {i+1}: è¨“ç·´é›†({train_end-train_start})æ­£æ¨£æœ¬={n_train_pos}ï¼Œæ¸¬è©¦é›†({test_end-test_start})æ­£æ¨£æœ¬={n_test_pos}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols_baseline, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"Is_laundering\")\n",
    "    test_data  = assembler.transform(test_df).select(\"features\", \"Is_laundering\")\n",
    "\n",
    "    lr = SparkLR(maxIter=100, regParam=0.01, labelCol=\"Is_laundering\", probabilityCol=\"probability\")\n",
    "    t1 = time.time()\n",
    "    model = lr.fit(train_data)\n",
    "    pred = model.transform(test_data)\n",
    "    t2 = time.time()\n",
    "\n",
    "    acc = evaluator.setMetricName(\"accuracy\").evaluate(pred)\n",
    "    prec = evaluator.setMetricName(\"weightedPrecision\").evaluate(pred)\n",
    "    rec = evaluator.setMetricName(\"weightedRecall\").evaluate(pred)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(pred)\n",
    "    metrics_base.append((acc, prec, rec, f1, t2-t1))\n",
    "\n",
    "    pred_pd = pred.select(\"Is_laundering\", \"prediction\", \"probability\").toPandas()\n",
    "    all_true_base.append(pred_pd[\"Is_laundering\"].values)\n",
    "    all_pred_base.append(pred_pd[\"prediction\"].values)\n",
    "    all_proba_base.append(pred_pd[\"probability\"].apply(lambda x: x[1] if hasattr(x, \"__getitem__\") else x).values)\n",
    "\n",
    "y_true_base = np.concatenate(all_true_base)\n",
    "y_pred_base = np.concatenate(all_pred_base)\n",
    "y_proba_base = np.concatenate(all_proba_base)\n",
    "metrics_base = np.array(metrics_base)\n",
    "\n",
    "print(\"\\nğŸ“Œ Baseline LRï¼ˆsliding window äº”æŠ˜é©—è­‰ï¼‰\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(metrics_base[:,0])), \n",
    "    (\"Precision\", np.mean(metrics_base[:,1])),\n",
    "    (\"Recall\", np.mean(metrics_base[:,2])),\n",
    "    (\"F1 Score\", np.mean(metrics_base[:,3])),\n",
    "    (\"Avg Time\", np.mean(metrics_base[:,4]))\n",
    "], headers=[\"æŒ‡æ¨™\", \"å¹³å‡å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== ç•«å‡º Baseline LR çš„ ROC æ›²ç·š ======\n",
    "fpr_b, tpr_b, _ = roc_curve(y_true_base, y_proba_base)\n",
    "roc_auc_b = auc(fpr_b, tpr_b)\n",
    "plt.plot(fpr_b, tpr_b, label=f'Baseline LR (AUC = {roc_auc_b:.4f})')\n",
    "\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: TD-UF+LR vs Baseline (Sliding Window 5-Fold)\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"both\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"âœ… å…¨éƒ¨æµç¨‹åŸ·è¡Œå®Œç•¢ï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
