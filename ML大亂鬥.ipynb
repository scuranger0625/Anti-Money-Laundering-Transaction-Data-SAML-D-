{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f11445",
   "metadata": {},
   "source": [
    "ä¸åŠ å…¥å¸³æˆ¶è³‡æ–™ åƒ…ä½¿ç”¨æœ€å¾Œ20%é€²è¡Œé©—è­‰ MLå¤§äº‚é¬¥ (æ™‚é–“æ˜¯åŸºæ–¼è·é›¢ä¸Šä¸€ç­†äº¤æ˜“çš„æ™‚é–“å·® è€Œä¸æ˜¯è·é›¢ç¬¬ä¸€ç­†çš„æ™‚é–“å·®)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb97a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š æ¨¡å‹æ•ˆèƒ½èˆ‡åŸ·è¡Œæ™‚é–“æ¯”è¼ƒï¼ˆä½¿ç”¨æœ€å¾Œ 20% ç‚ºæ¸¬è©¦é›†ï¼‰ï¼š\n",
      "\n",
      "ğŸ”¹ Logistic Regression\n",
      "   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š300.88 ç§’\n",
      "   ğŸ“ˆ AUC          ï¼š0.7632\n",
      "   ğŸ¯ Precision    ï¼š0.9989\n",
      "   ğŸ¯ Recall       ï¼š1.0000\n",
      "   ğŸ§® F1 Score     ï¼š0.9983\n",
      "\n",
      "ğŸ”¹ Decision Tree\n",
      "   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š298.85 ç§’\n",
      "   ğŸ“ˆ AUC          ï¼š0.3150\n",
      "   ğŸ¯ Precision    ï¼š0.9990\n",
      "   ğŸ¯ Recall       ï¼š1.0000\n",
      "   ğŸ§® F1 Score     ï¼š0.9985\n",
      "\n",
      "ğŸ”¹ Random Forest\n",
      "   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š1082.97 ç§’\n",
      "   ğŸ“ˆ AUC          ï¼š0.7420\n",
      "   ğŸ¯ Precision    ï¼š0.9989\n",
      "   ğŸ¯ Recall       ï¼š1.0000\n",
      "   ğŸ§® F1 Score     ï¼š0.9983\n",
      "\n",
      "ğŸ”¹ SVM (LinearSVC)\n",
      "   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š306.73 ç§’\n",
      "   ğŸ“ˆ AUC          ï¼š0.7523\n",
      "   ğŸ¯ Precision    ï¼š0.9989\n",
      "   ğŸ¯ Recall       ï¼š1.0000\n",
      "   ğŸ§® F1 Score     ï¼š0.9983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, concat_ws, lag, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier,\n",
    "    RandomForestClassifier, LinearSVC\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# å»ºç«‹ Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SAML-D Classifier Comparison with Timing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# è®€å–è³‡æ–™\n",
    "file_path = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# åŠ å…¥æ™‚é–“æ¬„ä½èˆ‡æ™‚é–“å·®\n",
    "df = df.withColumn(\"datetime\", unix_timestamp(concat_ws(\" \", df[\"Date\"], df[\"Time\"]), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "window_spec = Window.orderBy(\"datetime\")\n",
    "df = df.withColumn(\"prev_time\", lag(\"datetime\", 1).over(window_spec))\n",
    "df = df.withColumn(\"time_diff\", col(\"datetime\") - col(\"prev_time\")).fillna({\"time_diff\": 0})\n",
    "\n",
    "# é¡åˆ¥æ¬„ä½èˆ‡å‘é‡æ¬„ä½\n",
    "categorical_cols = [\n",
    "    \"Payment_currency\", \"Received_currency\",\n",
    "    \"Sender_bank_location\", \"Receiver_bank_location\",\n",
    "    \"Payment_type\"\n",
    "]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c + \"_idx\", outputCol=c + \"_vec\") for c in categorical_cols]\n",
    "feature_cols = [\"Amount\", \"time_diff\"] + [c + \"_vec\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# æ™‚é–“æ’åºåˆ‡åˆ†\n",
    "df_sorted = df.orderBy(\"datetime\")\n",
    "total_count = df_sorted.count()\n",
    "train_count = int(total_count * 0.8)\n",
    "train_data = df_sorted.limit(train_count)\n",
    "test_data = df_sorted.subtract(train_data)\n",
    "\n",
    "# æ¨¡å‹è¨­å®š\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"Is_laundering\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"Is_laundering\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"Is_laundering\", featuresCol=\"features\", numTrees=100),\n",
    "    \"SVM (LinearSVC)\": LinearSVC(labelCol=\"Is_laundering\", featuresCol=\"features\")\n",
    "}\n",
    "\n",
    "# è©•ä¼°å™¨\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=\"Is_laundering\", metricName=\"areaUnderROC\")\n",
    "precision_eval = MulticlassClassificationEvaluator(labelCol=\"Is_laundering\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "recall_eval = MulticlassClassificationEvaluator(labelCol=\"Is_laundering\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"Is_laundering\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# åŸ·è¡Œæ¨¡å‹èˆ‡è¨ˆæ™‚\n",
    "print(\"\\nğŸ“Š æ¨¡å‹æ•ˆèƒ½èˆ‡åŸ·è¡Œæ™‚é–“æ¯”è¼ƒï¼ˆä½¿ç”¨æœ€å¾Œ 20% ç‚ºæ¸¬è©¦é›†ï¼‰ï¼š\\n\")\n",
    "for name, clf in models.items():\n",
    "    print(f\"ğŸ”¹ {name}\")\n",
    "    start = time.time()\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler, clf])\n",
    "    model = pipeline.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    auc = binary_eval.evaluate(predictions)\n",
    "    precision = precision_eval.evaluate(predictions)\n",
    "    recall = recall_eval.evaluate(predictions)\n",
    "    f1 = f1_eval.evaluate(predictions)\n",
    "\n",
    "    print(f\"   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š{elapsed:.2f} ç§’\")\n",
    "    print(f\"   ğŸ“ˆ AUC          ï¼š{auc:.4f}\")\n",
    "    print(f\"   ğŸ¯ Precision    ï¼š{precision:.4f}\")\n",
    "    print(f\"   ğŸ¯ Recall       ï¼š{recall:.4f}\")\n",
    "    print(f\"   ğŸ§® F1 Score     ï¼š{f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1f7a1",
   "metadata": {},
   "source": [
    "æŒ‡æ¨™\tæ˜¯å¦è€ƒæ…®æ©Ÿç‡\tæ˜¯å¦å›ºå®š threshold\té©åˆè³‡æ–™ä¸å¹³è¡¡ï¼Ÿ\n",
    "Accuracy\tâŒ ä¸çœ‹æ©Ÿç‡\tâœ… å›ºå®šï¼ˆé€šå¸¸ç‚º0.5ï¼‰\tâŒ å®¹æ˜“èª¤å°\n",
    "AUC\tâœ… çœ‹æ©Ÿç‡æ’åº\tâŒ ä¸å›ºå®š\tâœ… ç©©å®š\n",
    "\n",
    "ä½¿ç”¨AUCå¯ä»¥é¿å…ä½¿ç”¨Accuracyå°è‡´çš„é æ¸¬ä¸æ˜¯æ´—éŒ¢è€Œè®Šæˆ99%çš„å•é¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27260d",
   "metadata": {},
   "source": [
    "æŒ‡æ¨™\tæ„ç¾©\tèˆ‡èª¤æ®ºï¼ˆèª¤åˆ¤æ­£å¸¸ç‚ºæ´—éŒ¢ï¼‰æœ‰é—œå—ï¼Ÿ\n",
    "AUC\tæ•´é«”æ’åºèƒ½åŠ›ï¼Œç„¡è«–é–€æª»\tâŒ ä¸ç›´æ¥å‘Šè¨´ä½ èª¤æ®ºç‡ï¼Œä½†å¥½çš„ AUC é€šå¸¸æœƒå¸¶ä¾†å¥½çš„ precision\n",
    "Precision\té æ¸¬ç‚ºæ´—éŒ¢ä¸­ï¼ŒçœŸçš„æ´—éŒ¢çš„æ¯”ä¾‹\tâœ… ç›´æ¥åæ˜ ã€Œèª¤æ®ºå¤šä¸å¤šã€\n",
    "Recall\tæ‰€æœ‰æ´—éŒ¢ä¸­ï¼Œæ¨¡å‹æŠ“åˆ°å¹¾å€‹\tâœ… è¶Šé«˜è¶Šèƒ½ç™¼ç¾è©æ¬ºï¼Œä½†å¯èƒ½æœƒçŠ§ç‰² precisionï¼ˆé€ æˆèª¤æ®ºï¼‰\n",
    "F1 Score\tPrecision + Recall çš„å¹³è¡¡\tâœ… ç¶œåˆè¡¨ç¾å¥½å£çš„é‡è¦æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705be3d9",
   "metadata": {},
   "source": [
    "ç­‰ç­‰è€ƒæ…®ä½¿ç”¨å¤šæ¨¡æ…‹+MLå¤§äº‚é¬¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
