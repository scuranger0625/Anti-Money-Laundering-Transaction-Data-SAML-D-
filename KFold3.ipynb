{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d888f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹• SparkSession...\n",
      "âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\n",
      "ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\n",
      "âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± 9504852 ç­†äº¤æ˜“\n",
      "ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9504852/9504852 [00:11<00:00, 812407.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     72\u001b[0m tx_pd \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs_laundering\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     75\u001b[0m tx_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoh\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poh_chain(tx_pd\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_pd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m tx \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs_laundering\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:362\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    360\u001b[0m             warn(msg)\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:520\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[1;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mastype(record_dtype)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Convert list of numpy records to python lists\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:520\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mastype(record_dtype)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Convert list of numpy records to python lists\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:382\u001b[0m, in \u001b[0;36mSparkContext._do_init.<locals>.signal_handler\u001b[1;34m(signal, frame)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msignal_handler\u001b[39m(signal: Any, frame: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancelAllJobs()\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, lag, collect_list\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from graphframes import GraphFrame\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg, stddev, count\n",
    "from pyspark.sql.functions import row_number\n",
    "import glob\n",
    "\n",
    "# ====== Step 0: å¼·åˆ¶è¨­å®š Java/Hadoop è·¯å¾‘ï¼ˆWindowsï¼‰ ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== Step 0: å»ºç«‹ SparkSession ======\n",
    "print(\"ğŸš€ å•Ÿå‹• SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF (GraphFrames) vs Logistic Regression\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\")\n",
    "\n",
    "# ====== Step 1: æ—¥æœŸå­—ä¸²è½‰ epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== Step 2: è®€å–è³‡æ–™ï¼ŒåŠ  tx_idã€timestamp ======\n",
    "print(\"ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± {df.count()} ç­†äº¤æ˜“\")\n",
    "\n",
    "# ====== Step 3: è¨ˆç®— PoH éˆå¼é›œæ¹Š ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"âœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\")\n",
    "\n",
    "tx = df.select(\"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\", \"poh\")\n",
    "\n",
    "# ====== Step 3.5: TD-UF åˆ†ç¾¤ + èšåˆç¸½é‹ç®—æ™‚é–“ï¼ˆè¨˜éŒ„ runtimeï¼‰ ======\n",
    "t_tduf_start = time.time()\n",
    "\n",
    "# ====== TD-UF åˆ†ç¾¤ ======\n",
    "print(\"ğŸ§© TD-UF æ­¥é©Ÿï¼šå»ºç«‹äº¤æ˜“åœ–çš„é ‚é»èˆ‡é‚Š ...\")\n",
    "vertices = tx.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "edges_df = (\n",
    "    tx.alias(\"a\").join(tx.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\")\n",
    "    .select(col(\"a.tx_id\").alias(\"src\"), col(\"b.tx_id\").alias(\"dst\"))\n",
    "    .distinct()\n",
    ")\n",
    "print(f\"âœ”ï¸ é ‚é»æ•¸: {vertices.count()}ï¼Œé‚Šæ•¸: {edges_df.count()}\")\n",
    "\n",
    "print(\"ğŸ”— TD-UF æ­¥é©Ÿï¼šè¨ˆç®—åˆ†ç¾¤ï¼ˆconnectedComponentsï¼‰ ...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"âœ”ï¸ å®Œæˆåˆ†ç¾¤è¨ˆç®—\")\n",
    "\n",
    "print(\"ğŸ”— TD-UF æ­¥é©Ÿï¼šJoin åŸå§‹äº¤æ˜“ ...\")\n",
    "dfc = components.join(tx, components.id == tx.tx_id)\n",
    "print(f\"âœ”ï¸ Join å®Œæˆï¼Œè³‡æ–™ç­†æ•¸: {dfc.count()}\")\n",
    "\n",
    "# ====== Step 4: èšåˆç‰¹å¾µ ======\n",
    "print(\"ğŸ“Š ç¾¤çµ„å…§èšåˆç‰¹å¾µä¸­ ...\")\n",
    "w = Window.partitionBy(\"component\").orderBy(\"timestamp\")\n",
    "dfc = (dfc.withColumn(\"prev_ts\", lag(\"timestamp\", 1).over(w))\n",
    "           .withColumn(\"gap\", col(\"timestamp\") - col(\"prev_ts\")))\n",
    "\n",
    "agg_df = (\n",
    "    dfc.groupBy(\"component\")\n",
    "    .agg(\n",
    "        spark_min(\"timestamp\").alias(\"start_ts\"),\n",
    "        count(\"timestamp\").alias(\"tx_count\"),\n",
    "        avg(\"Amount\").alias(\"avg_amt\"),\n",
    "        stddev(\"Amount\").alias(\"std_amt\"),\n",
    "        spark_max(\"Amount\").alias(\"max_amt\"),\n",
    "        spark_min(\"Amount\").alias(\"min_amt\"),\n",
    "        avg(\"gap\").alias(\"avg_gap\"),\n",
    "        stddev(\"gap\").alias(\"std_gap\"),\n",
    "        spark_max(\"gap\").alias(\"max_gap\"),\n",
    "        spark_min(\"gap\").alias(\"min_gap\"),\n",
    "        spark_max(\"Is_laundering\").alias(\"label\"),\n",
    "        collect_list(\"poh\").alias(\"poh_hashes\")\n",
    "    ).fillna({\"std_amt\": 0, \"std_gap\": 0})\n",
    ")\n",
    "print(\"âœ”ï¸ ç¾¤çµ„çµ±è¨ˆå®Œæˆ\")\n",
    "t_tduf_end = time.time()\n",
    "print(f\"â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆé‹ç®—æ™‚é–“ï¼š{t_tduf_end - t_tduf_start:.2f} ç§’\")   # <--- è¨ˆæ™‚\n",
    "\n",
    "# ====== Step 5: åˆ†æ‰¹ä¸‹è¼‰ç‰¹å¾µè³‡æ–™åˆ° pandas ======\n",
    "print(\"â¬‡ï¸ åˆ†æ‰¹ä¸‹è¼‰ç‰¹å¾µè³‡æ–™åˆ° pandas ...\")\n",
    "BATCH_SIZE = 10000\n",
    "window = Window.orderBy(\"component\")\n",
    "agg_df = agg_df.withColumn(\"rn\", row_number().over(window))\n",
    "total_count = agg_df.count()\n",
    "num_batches = (total_count + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * BATCH_SIZE + 1\n",
    "    end = min((i + 1) * BATCH_SIZE, total_count)\n",
    "    print(f\"â³ Collect batch {i+1}/{num_batches} ... [{start} ~ {end}]\")\n",
    "    batch = agg_df.filter((col(\"rn\") >= start) & (col(\"rn\") <= end)).drop(\"rn\")\n",
    "    batch_pd = batch.toPandas()\n",
    "    batch_pd[\"poh_hashes\"] = batch_pd[\"poh_hashes\"].apply(lambda x: \";\".join(map(str, x)))\n",
    "    batch_pd.to_csv(f\"tduf_group_features_batch{i+1}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ”ï¸ è¼¸å‡º batch {i+1} å®Œæˆï¼Œå…± {len(batch_pd)} ç­†\")\n",
    "\n",
    "print(\"âœ”ï¸ å…¨éƒ¨åˆ†æ‰¹åŒ¯å‡ºå®Œæˆ\")\n",
    "\n",
    "# ====== Step 5.5: åˆä½µæ‰€æœ‰ batch csv ======\n",
    "print(\"ğŸ—ƒï¸ åˆä½µæ‰€æœ‰ batch csv æª”æ¡ˆ ...\")\n",
    "csv_list = sorted(glob.glob(\"tduf_group_features_batch*.csv\"))\n",
    "pd_list = [pd.read_csv(f) for f in csv_list]\n",
    "agg_pd = pd.concat(pd_list, ignore_index=True)\n",
    "print(f\"âœ”ï¸ åˆä½µå®Œæˆï¼Œå…± {len(agg_pd)} ç­†ç¾¤çµ„\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"tx_count\", \"avg_amt\", \"std_amt\", \"max_amt\", \"min_amt\",\n",
    "    \"avg_gap\", \"std_gap\", \"max_gap\", \"min_gap\"\n",
    "]\n",
    "X = agg_pd[feature_cols].values\n",
    "y = agg_pd[\"label\"].values\n",
    "\n",
    "# ====== Step 6: åæŠ˜åˆ†å±¤äº¤å‰é©—è­‰ï¼ˆ**è¨˜éŒ„ LR è¨“ç·´/é æ¸¬ runtime**ï¼‰ ======\n",
    "print(\"ğŸ”„ åæŠ˜åˆ†å±¤äº¤å‰é©—è­‰ä¸­ ...\")\n",
    "t_lr_start = time.time()\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "agg_pd[\"cv_fold\"] = -1\n",
    "cv_pred = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"  ğŸš© Fold {fold}/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf = SklearnLR(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    precs.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recs.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "    f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "    agg_pd.loc[test_idx, \"cv_fold\"] = fold\n",
    "    for idx, p in zip(test_idx, y_pred):\n",
    "        cv_pred.append((agg_pd.iloc[idx][\"component\"], int(agg_pd.iloc[idx][\"label\"]), int(p), fold))\n",
    "t_lr_end = time.time()\n",
    "\n",
    "print(\"\\nğŸ“Œ TD-UF (GraphFrames) + LR (StratifiedKFold)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(accs)), \n",
    "    (\"Precision\", np.mean(precs)),\n",
    "    (\"Recall\", np.mean(recs)),\n",
    "    (\"F1 Score\", np.mean(f1s))\n",
    "], headers=[\"æŒ‡æ¨™\", \"å¹³å‡å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(f\"â±ï¸ Sklearn LR åæŠ˜è¨“ç·´+æ¨è«–é‹ç®—æ™‚é–“ï¼š{t_lr_end - t_lr_start:.2f} ç§’\")\n",
    "print(f\"â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆ + Sklearn LR ç¸½é‹ç®—æ™‚é–“ï¼š{(t_tduf_end - t_tduf_start) + (t_lr_end - t_lr_start):.2f} ç§’\")   # <--- åŠ ç¸½\n",
    "\n",
    "# ====== è¼¸å‡ºé©—è­‰çµæœ ======\n",
    "print(\"ğŸ’¾ è¼¸å‡ºç¾¤çµ„ç‰¹å¾µèˆ‡é©—è­‰çµæœ CSV ...\")\n",
    "agg_pd[\"cv_pred\"] = [p for _, _, p, _ in sorted(cv_pred, key=lambda x: x[0])]\n",
    "agg_pd.to_csv(\"tduf_group_features_with_poh_graphframes.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(\"tduf_10fold_metrics_graphframes.csv\", \"w\", newline='', encoding=\"utf-8-sig\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"fold\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    for i, (a, p, r, f1) in enumerate(zip(accs, precs, recs, f1s), 1):\n",
    "        w.writerow([i, a, p, r, f1])\n",
    "    w.writerow([\"avg\", np.mean(accs), np.mean(precs), np.mean(recs), np.mean(f1s)])\n",
    "print(\"âœ”ï¸ æ‰€æœ‰è¼¸å‡ºå·²å®Œæˆ\")\n",
    "\n",
    "# ====== Step 7: Baselineï¼ˆå…¨æ¬„ä½ LR, ç”¨ PySparkï¼‰ ======\n",
    "print(\"ğŸš© Baseline å…¨ç‰¹å¾µé‚è¼¯è¿´æ­¸è¨“ç·´ä¸­ ...\")\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df) for c in categorical_cols]\n",
    "for indexer in tqdm(indexers, desc=\"Label encodingï¼ˆåˆ†é¡æ¬„ä½ï¼‰\"):\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "feature_cols_full = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_full, outputCol=\"features\")\n",
    "vector_df = assembler.transform(df.withColumn(\"label\", col(\"Is_laundering\"))).select(\"features\", \"label\")\n",
    "train_b, val_b, test_b = vector_df.randomSplit([0.7, 0.2, 0.1], seed=42)\n",
    "\n",
    "lr = SparkLR(maxIter=100, regParam=0.01)\n",
    "t2 = time.time()\n",
    "model_base = lr.fit(train_b)\n",
    "result_base = model_base.transform(test_b)\n",
    "t3 = time.time()\n",
    "print(f\"â±ï¸ Baseline LR (PySpark) è¨“ç·´èˆ‡æ¨è«–é‹ç®—æ™‚é–“: {t3 - t2:.2f} ç§’\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc_base = evaluator.setMetricName(\"accuracy\").evaluate(result_base)\n",
    "f1_base  = evaluator.setMetricName(\"f1\").evaluate(result_base)\n",
    "prec_base = evaluator.setMetricName(\"weightedPrecision\").evaluate(result_base)\n",
    "rec_base = evaluator.setMetricName(\"weightedRecall\").evaluate(result_base)\n",
    "\n",
    "print(\"\\nğŸ“Œ Baseline LR (PySpark)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", acc_base), \n",
    "    (\"Precision\", prec_base), \n",
    "    (\"Recall\", rec_base), \n",
    "    (\"F1 Score\", f1_base), \n",
    "    (\"Time\", t3 - t2)\n",
    "], headers=[\"æŒ‡æ¨™\", \"æ•¸å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== Step 8: ç²¾æº–åº¦æ¯”è¼ƒåœ–è¡¨ ======\n",
    "print(\"ğŸ“Š ç•«å‡º TD-UF èˆ‡ Baseline ç²¾æº–åº¦æ¯”è¼ƒåœ– ...\")\n",
    "labels = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x - 0.2, [np.mean(precs), np.mean(recs), np.mean(f1s)], width=0.4, label=\"TD-UF + LR\")\n",
    "plt.bar(x + 0.2, [prec_base, rec_base, f1_base], width=0.4, label=\"Baseline LR\")\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"TD-UF (GraphFrames) vs Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"âœ… å…¨éƒ¨æµç¨‹åŸ·è¡Œå®Œç•¢ï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
