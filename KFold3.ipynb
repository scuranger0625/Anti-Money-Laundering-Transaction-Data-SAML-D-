{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d888f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動 SparkSession...\n",
      "✔️ SparkSession 建立完成\n",
      "📥 讀取資料中 ...\n",
      "✔️ 載入資料完成，共 9504852 筆交易\n",
      "🔄 產生 PoH 鏈式雜湊欄位 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 產生 PoH 雜湊鏈中...: 100%|██████████| 9504852/9504852 [00:11<00:00, 812407.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     72\u001b[0m tx_pd \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs_laundering\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     75\u001b[0m tx_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoh\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poh_chain(tx_pd\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_pd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔️ PoH 雜湊計算完成\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m tx \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs_laundering\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:362\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    360\u001b[0m             warn(msg)\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:520\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[1;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mastype(record_dtype)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Convert list of numpy records to python lists\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:520\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mastype(record_dtype)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Convert list of numpy records to python lists\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m np_records]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:382\u001b[0m, in \u001b[0;36mSparkContext._do_init.<locals>.signal_handler\u001b[1;34m(signal, frame)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msignal_handler\u001b[39m(signal: Any, frame: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancelAllJobs()\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, lag, collect_list\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from graphframes import GraphFrame\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg, stddev, count\n",
    "from pyspark.sql.functions import row_number\n",
    "import glob\n",
    "\n",
    "# ====== Step 0: 強制設定 Java/Hadoop 路徑（Windows） ======\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== Step 0: 建立 SparkSession ======\n",
    "print(\"🚀 啟動 SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF (GraphFrames) vs Logistic Regression\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"✔️ SparkSession 建立完成\")\n",
    "\n",
    "# ====== Step 1: 日期字串轉 epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== Step 2: 讀取資料，加 tx_id、timestamp ======\n",
    "print(\"📥 讀取資料中 ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\程式語言資料\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"✔️ 載入資料完成，共 {df.count()} 筆交易\")\n",
    "\n",
    "# ====== Step 3: 計算 PoH 鏈式雜湊 ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"🔗 產生 PoH 雜湊鏈中...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"🔄 產生 PoH 鏈式雜湊欄位 ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"✔️ PoH 雜湊計算完成\")\n",
    "\n",
    "tx = df.select(\"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\", \"poh\")\n",
    "\n",
    "# ====== Step 3.5: TD-UF 分群 + 聚合總運算時間（記錄 runtime） ======\n",
    "t_tduf_start = time.time()\n",
    "\n",
    "# ====== TD-UF 分群 ======\n",
    "print(\"🧩 TD-UF 步驟：建立交易圖的頂點與邊 ...\")\n",
    "vertices = tx.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "edges_df = (\n",
    "    tx.alias(\"a\").join(tx.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\")\n",
    "    .select(col(\"a.tx_id\").alias(\"src\"), col(\"b.tx_id\").alias(\"dst\"))\n",
    "    .distinct()\n",
    ")\n",
    "print(f\"✔️ 頂點數: {vertices.count()}，邊數: {edges_df.count()}\")\n",
    "\n",
    "print(\"🔗 TD-UF 步驟：計算分群（connectedComponents） ...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"✔️ 完成分群計算\")\n",
    "\n",
    "print(\"🔗 TD-UF 步驟：Join 原始交易 ...\")\n",
    "dfc = components.join(tx, components.id == tx.tx_id)\n",
    "print(f\"✔️ Join 完成，資料筆數: {dfc.count()}\")\n",
    "\n",
    "# ====== Step 4: 聚合特徵 ======\n",
    "print(\"📊 群組內聚合特徵中 ...\")\n",
    "w = Window.partitionBy(\"component\").orderBy(\"timestamp\")\n",
    "dfc = (dfc.withColumn(\"prev_ts\", lag(\"timestamp\", 1).over(w))\n",
    "           .withColumn(\"gap\", col(\"timestamp\") - col(\"prev_ts\")))\n",
    "\n",
    "agg_df = (\n",
    "    dfc.groupBy(\"component\")\n",
    "    .agg(\n",
    "        spark_min(\"timestamp\").alias(\"start_ts\"),\n",
    "        count(\"timestamp\").alias(\"tx_count\"),\n",
    "        avg(\"Amount\").alias(\"avg_amt\"),\n",
    "        stddev(\"Amount\").alias(\"std_amt\"),\n",
    "        spark_max(\"Amount\").alias(\"max_amt\"),\n",
    "        spark_min(\"Amount\").alias(\"min_amt\"),\n",
    "        avg(\"gap\").alias(\"avg_gap\"),\n",
    "        stddev(\"gap\").alias(\"std_gap\"),\n",
    "        spark_max(\"gap\").alias(\"max_gap\"),\n",
    "        spark_min(\"gap\").alias(\"min_gap\"),\n",
    "        spark_max(\"Is_laundering\").alias(\"label\"),\n",
    "        collect_list(\"poh\").alias(\"poh_hashes\")\n",
    "    ).fillna({\"std_amt\": 0, \"std_gap\": 0})\n",
    ")\n",
    "print(\"✔️ 群組統計完成\")\n",
    "t_tduf_end = time.time()\n",
    "print(f\"⏱️ TD-UF 分群 + 特徵聚合運算時間：{t_tduf_end - t_tduf_start:.2f} 秒\")   # <--- 計時\n",
    "\n",
    "# ====== Step 5: 分批下載特徵資料到 pandas ======\n",
    "print(\"⬇️ 分批下載特徵資料到 pandas ...\")\n",
    "BATCH_SIZE = 10000\n",
    "window = Window.orderBy(\"component\")\n",
    "agg_df = agg_df.withColumn(\"rn\", row_number().over(window))\n",
    "total_count = agg_df.count()\n",
    "num_batches = (total_count + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * BATCH_SIZE + 1\n",
    "    end = min((i + 1) * BATCH_SIZE, total_count)\n",
    "    print(f\"⏳ Collect batch {i+1}/{num_batches} ... [{start} ~ {end}]\")\n",
    "    batch = agg_df.filter((col(\"rn\") >= start) & (col(\"rn\") <= end)).drop(\"rn\")\n",
    "    batch_pd = batch.toPandas()\n",
    "    batch_pd[\"poh_hashes\"] = batch_pd[\"poh_hashes\"].apply(lambda x: \";\".join(map(str, x)))\n",
    "    batch_pd.to_csv(f\"tduf_group_features_batch{i+1}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✔️ 輸出 batch {i+1} 完成，共 {len(batch_pd)} 筆\")\n",
    "\n",
    "print(\"✔️ 全部分批匯出完成\")\n",
    "\n",
    "# ====== Step 5.5: 合併所有 batch csv ======\n",
    "print(\"🗃️ 合併所有 batch csv 檔案 ...\")\n",
    "csv_list = sorted(glob.glob(\"tduf_group_features_batch*.csv\"))\n",
    "pd_list = [pd.read_csv(f) for f in csv_list]\n",
    "agg_pd = pd.concat(pd_list, ignore_index=True)\n",
    "print(f\"✔️ 合併完成，共 {len(agg_pd)} 筆群組\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"tx_count\", \"avg_amt\", \"std_amt\", \"max_amt\", \"min_amt\",\n",
    "    \"avg_gap\", \"std_gap\", \"max_gap\", \"min_gap\"\n",
    "]\n",
    "X = agg_pd[feature_cols].values\n",
    "y = agg_pd[\"label\"].values\n",
    "\n",
    "# ====== Step 6: 十折分層交叉驗證（**記錄 LR 訓練/預測 runtime**） ======\n",
    "print(\"🔄 十折分層交叉驗證中 ...\")\n",
    "t_lr_start = time.time()\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "agg_pd[\"cv_fold\"] = -1\n",
    "cv_pred = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"  🚩 Fold {fold}/10 訓練與測試 ...\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf = SklearnLR(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    precs.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recs.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "    f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "    agg_pd.loc[test_idx, \"cv_fold\"] = fold\n",
    "    for idx, p in zip(test_idx, y_pred):\n",
    "        cv_pred.append((agg_pd.iloc[idx][\"component\"], int(agg_pd.iloc[idx][\"label\"]), int(p), fold))\n",
    "t_lr_end = time.time()\n",
    "\n",
    "print(\"\\n📌 TD-UF (GraphFrames) + LR (StratifiedKFold)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(accs)), \n",
    "    (\"Precision\", np.mean(precs)),\n",
    "    (\"Recall\", np.mean(recs)),\n",
    "    (\"F1 Score\", np.mean(f1s))\n",
    "], headers=[\"指標\", \"平均值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(f\"⏱️ Sklearn LR 十折訓練+推論運算時間：{t_lr_end - t_lr_start:.2f} 秒\")\n",
    "print(f\"⏱️ TD-UF 分群 + 特徵聚合 + Sklearn LR 總運算時間：{(t_tduf_end - t_tduf_start) + (t_lr_end - t_lr_start):.2f} 秒\")   # <--- 加總\n",
    "\n",
    "# ====== 輸出驗證結果 ======\n",
    "print(\"💾 輸出群組特徵與驗證結果 CSV ...\")\n",
    "agg_pd[\"cv_pred\"] = [p for _, _, p, _ in sorted(cv_pred, key=lambda x: x[0])]\n",
    "agg_pd.to_csv(\"tduf_group_features_with_poh_graphframes.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(\"tduf_10fold_metrics_graphframes.csv\", \"w\", newline='', encoding=\"utf-8-sig\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"fold\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    for i, (a, p, r, f1) in enumerate(zip(accs, precs, recs, f1s), 1):\n",
    "        w.writerow([i, a, p, r, f1])\n",
    "    w.writerow([\"avg\", np.mean(accs), np.mean(precs), np.mean(recs), np.mean(f1s)])\n",
    "print(\"✔️ 所有輸出已完成\")\n",
    "\n",
    "# ====== Step 7: Baseline（全欄位 LR, 用 PySpark） ======\n",
    "print(\"🚩 Baseline 全特徵邏輯迴歸訓練中 ...\")\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df) for c in categorical_cols]\n",
    "for indexer in tqdm(indexers, desc=\"Label encoding（分類欄位）\"):\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "feature_cols_full = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_full, outputCol=\"features\")\n",
    "vector_df = assembler.transform(df.withColumn(\"label\", col(\"Is_laundering\"))).select(\"features\", \"label\")\n",
    "train_b, val_b, test_b = vector_df.randomSplit([0.7, 0.2, 0.1], seed=42)\n",
    "\n",
    "lr = SparkLR(maxIter=100, regParam=0.01)\n",
    "t2 = time.time()\n",
    "model_base = lr.fit(train_b)\n",
    "result_base = model_base.transform(test_b)\n",
    "t3 = time.time()\n",
    "print(f\"⏱️ Baseline LR (PySpark) 訓練與推論運算時間: {t3 - t2:.2f} 秒\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc_base = evaluator.setMetricName(\"accuracy\").evaluate(result_base)\n",
    "f1_base  = evaluator.setMetricName(\"f1\").evaluate(result_base)\n",
    "prec_base = evaluator.setMetricName(\"weightedPrecision\").evaluate(result_base)\n",
    "rec_base = evaluator.setMetricName(\"weightedRecall\").evaluate(result_base)\n",
    "\n",
    "print(\"\\n📌 Baseline LR (PySpark)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", acc_base), \n",
    "    (\"Precision\", prec_base), \n",
    "    (\"Recall\", rec_base), \n",
    "    (\"F1 Score\", f1_base), \n",
    "    (\"Time\", t3 - t2)\n",
    "], headers=[\"指標\", \"數值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== Step 8: 精準度比較圖表 ======\n",
    "print(\"📊 畫出 TD-UF 與 Baseline 精準度比較圖 ...\")\n",
    "labels = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x - 0.2, [np.mean(precs), np.mean(recs), np.mean(f1s)], width=0.4, label=\"TD-UF + LR\")\n",
    "plt.bar(x + 0.2, [prec_base, rec_base, f1_base], width=0.4, label=\"Baseline LR\")\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"TD-UF (GraphFrames) vs Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"✅ 全部流程執行完畢！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
