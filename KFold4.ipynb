{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400163af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹• SparkSession...\n",
      "âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\n",
      "ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\n",
      "âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± 9504852 ç­†äº¤æ˜“\n",
      "ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9504852/9504852 [00:17<00:00, 538917.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\n",
      "ğŸ§© TD-UF æ­¥é©Ÿï¼šå»ºç«‹äº¤æ˜“åœ–çš„é ‚é»èˆ‡é‚Š ...\n",
      "âœ”ï¸ é ‚é»æ•¸: 9504852ï¼Œé‚Šæ•¸: 0\n",
      "ğŸ”— TD-UF æ­¥é©Ÿï¼šè¨ˆç®—åˆ†ç¾¤ï¼ˆconnectedComponentsï¼‰ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ å®Œæˆåˆ†ç¾¤è¨ˆç®—\n",
      "ğŸ”— TD-UF æ­¥é©Ÿï¼šJoin åŸå§‹äº¤æ˜“ ...\n",
      "âœ”ï¸ Join å®Œæˆï¼Œè³‡æ–™ç­†æ•¸: 9504852\n",
      "ğŸ“Š ç¾¤çµ„å…§èšåˆç‰¹å¾µä¸­ ...\n",
      "âœ”ï¸ ç¾¤çµ„çµ±è¨ˆå®Œæˆ\n",
      "â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆé‹ç®—æ™‚é–“ï¼š92.99 ç§’\n",
      "ğŸšš ä¸€æ¬¡ collect åˆ° pandas ...ï¼ˆç”¨å®Œè‡ªå‹•åˆªï¼‰\n",
      "â±ï¸ toPandas() é‹ç®—æ™‚é–“ï¼š147.96 ç§’\n",
      "âœ”ï¸ æš«å­˜æª”æ¡ˆå„²å­˜å®Œæˆ (tduf_group_features_temp.csv) å…± 9504852 ç­†\n",
      "ğŸ”„ åæŠ˜åˆ†å±¤äº¤å‰é©—è­‰ä¸­ ...\n",
      "  ğŸš© Fold 1/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 2/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 3/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 4/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 5/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 6/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 7/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 8/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 9/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "  ğŸš© Fold 10/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\n",
      "\n",
      "ğŸ“Œ TD-UF (GraphFrames) + LR (StratifiedKFold)\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ æŒ‡æ¨™      â”‚     å¹³å‡å€¼ â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Accuracy  â”‚ 0.998966   â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Precision â”‚ 1          â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Recall    â”‚ 0.00465925 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ F1 Score  â”‚ 0.00926792 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "â±ï¸ Sklearn LR åæŠ˜è¨“ç·´+æ¨è«–é‹ç®—æ™‚é–“ï¼š884.13 ç§’\n",
      "â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆ + Sklearn LR ç¸½é‹ç®—æ™‚é–“ï¼š977.12 ç§’\n",
      "ğŸ§¹ å·²è‡ªå‹•åˆªé™¤æš«å­˜ CSVï¼štduf_group_features_temp.csv\n",
      "ğŸš© Baseline å…¨ç‰¹å¾µé‚è¼¯è¿´æ­¸è¨“ç·´ä¸­ ...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o232.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš© Baseline å…¨ç‰¹å¾µé‚è¼¯è¿´æ­¸è¨“ç·´ä¸­ ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 196\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mc, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(df) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[0;32m    198\u001b[0m     df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš© Baseline å…¨ç‰¹å¾µé‚è¼¯è¿´æ­¸è¨“ç·´ä¸­ ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 196\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [\u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[0;32m    198\u001b[0m     df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o232.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, lag, collect_list\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg, stddev, count\n",
    "from graphframes import GraphFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# å¼·åˆ¶è¨­å®š Java/Hadoop è·¯å¾‘\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== Step 0: å»ºç«‹ SparkSession ======\n",
    "print(\"ğŸš€ å•Ÿå‹• SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF (GraphFrames) vs Logistic Regression\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"âœ”ï¸ SparkSession å»ºç«‹å®Œæˆ\")\n",
    "\n",
    "# ====== Step 1: æ—¥æœŸå­—ä¸²è½‰ epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== Step 2: è®€å–è³‡æ–™ï¼ŒåŠ  tx_idã€timestamp ======\n",
    "print(\"ğŸ“¥ è®€å–è³‡æ–™ä¸­ ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\ç¨‹å¼èªè¨€è³‡æ–™\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"âœ”ï¸ è¼‰å…¥è³‡æ–™å®Œæˆï¼Œå…± {df.count()} ç­†äº¤æ˜“\")\n",
    "\n",
    "# ====== Step 3: è¨ˆç®— PoH éˆå¼é›œæ¹Š ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"ğŸ”— ç”¢ç”Ÿ PoH é›œæ¹Šéˆä¸­...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"ğŸ”„ ç”¢ç”Ÿ PoH éˆå¼é›œæ¹Šæ¬„ä½ ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"âœ”ï¸ PoH é›œæ¹Šè¨ˆç®—å®Œæˆ\")\n",
    "\n",
    "tx = df.select(\"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\", \"poh\")\n",
    "\n",
    "# ====== Step 3.5: TD-UF åˆ†ç¾¤ + èšåˆç¸½é‹ç®—æ™‚é–“ï¼ˆè¨˜éŒ„ runtimeï¼‰ ======\n",
    "t_tduf_start = time.time()\n",
    "\n",
    "# ====== TD-UF åˆ†ç¾¤ ======\n",
    "print(\"ğŸ§© TD-UF æ­¥é©Ÿï¼šå»ºç«‹äº¤æ˜“åœ–çš„é ‚é»èˆ‡é‚Š ...\")\n",
    "vertices = tx.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "edges_df = (\n",
    "    tx.alias(\"a\").join(tx.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\")\n",
    "    .select(col(\"a.tx_id\").alias(\"src\"), col(\"b.tx_id\").alias(\"dst\"))\n",
    "    .distinct()\n",
    ")\n",
    "print(f\"âœ”ï¸ é ‚é»æ•¸: {vertices.count()}ï¼Œé‚Šæ•¸: {edges_df.count()}\")\n",
    "\n",
    "print(\"ğŸ”— TD-UF æ­¥é©Ÿï¼šè¨ˆç®—åˆ†ç¾¤ï¼ˆconnectedComponentsï¼‰ ...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"âœ”ï¸ å®Œæˆåˆ†ç¾¤è¨ˆç®—\")\n",
    "\n",
    "print(\"ğŸ”— TD-UF æ­¥é©Ÿï¼šJoin åŸå§‹äº¤æ˜“ ...\")\n",
    "dfc = components.join(tx, components.id == tx.tx_id)\n",
    "print(f\"âœ”ï¸ Join å®Œæˆï¼Œè³‡æ–™ç­†æ•¸: {dfc.count()}\")\n",
    "\n",
    "# ====== Step 4: èšåˆç‰¹å¾µ ======\n",
    "print(\"ğŸ“Š ç¾¤çµ„å…§èšåˆç‰¹å¾µä¸­ ...\")\n",
    "w = Window.partitionBy(\"component\").orderBy(\"timestamp\")\n",
    "dfc = (dfc.withColumn(\"prev_ts\", lag(\"timestamp\", 1).over(w))\n",
    "           .withColumn(\"gap\", col(\"timestamp\") - col(\"prev_ts\")))\n",
    "\n",
    "agg_df = (\n",
    "    dfc.groupBy(\"component\")\n",
    "    .agg(\n",
    "        spark_min(\"timestamp\").alias(\"start_ts\"),\n",
    "        count(\"timestamp\").alias(\"tx_count\"),\n",
    "        avg(\"Amount\").alias(\"avg_amt\"),\n",
    "        stddev(\"Amount\").alias(\"std_amt\"),\n",
    "        spark_max(\"Amount\").alias(\"max_amt\"),\n",
    "        spark_min(\"Amount\").alias(\"min_amt\"),\n",
    "        avg(\"gap\").alias(\"avg_gap\"),\n",
    "        stddev(\"gap\").alias(\"std_gap\"),\n",
    "        spark_max(\"gap\").alias(\"max_gap\"),\n",
    "        spark_min(\"gap\").alias(\"min_gap\"),\n",
    "        spark_max(\"Is_laundering\").alias(\"label\"),\n",
    "        collect_list(\"poh\").alias(\"poh_hashes\")\n",
    "    ).fillna({\"std_amt\": 0, \"std_gap\": 0})\n",
    ")\n",
    "print(\"âœ”ï¸ ç¾¤çµ„çµ±è¨ˆå®Œæˆ\")\n",
    "t_tduf_end = time.time()\n",
    "print(f\"â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆé‹ç®—æ™‚é–“ï¼š{t_tduf_end - t_tduf_start:.2f} ç§’\")   # <--- è¨ˆæ™‚\n",
    "\n",
    "# ====== Step 5: ä¸€æ¬¡ collect ======\n",
    "print(\"ğŸšš ä¸€æ¬¡ collect åˆ° pandas ...ï¼ˆç”¨å®Œè‡ªå‹•åˆªï¼‰\")\n",
    "t_collect_start = time.time()\n",
    "agg_pd = agg_df.toPandas()\n",
    "t_collect_end = time.time()\n",
    "print(f\"â±ï¸ toPandas() é‹ç®—æ™‚é–“ï¼š{t_collect_end - t_collect_start:.2f} ç§’\")\n",
    "\n",
    "# ==== å„²å­˜æš«å­˜æª”æ¡ˆ ====\n",
    "TEMP_CSV = \"tduf_group_features_temp.csv\"\n",
    "agg_pd[\"poh_hashes\"] = agg_pd[\"poh_hashes\"].apply(lambda x: \";\".join(map(str, x)))\n",
    "agg_pd.to_csv(TEMP_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ”ï¸ æš«å­˜æª”æ¡ˆå„²å­˜å®Œæˆ ({TEMP_CSV}) å…± {len(agg_pd)} ç­†\")\n",
    "\n",
    "# ====== Step 6: è®€å–æš«å­˜æª”æ¡ˆï¼ŒåšåæŠ˜äº¤å‰é©—è­‰èˆ‡é‚è¼¯è¿´æ­¸è¨ˆæ™‚ ======\n",
    "print(\"ğŸ”„ åæŠ˜åˆ†å±¤äº¤å‰é©—è­‰ä¸­ ...\")\n",
    "t_lr_start = time.time()\n",
    "agg_pd = pd.read_csv(TEMP_CSV)\n",
    "agg_pd = agg_pd.fillna(0)\n",
    "feature_cols = [\n",
    "    \"tx_count\", \"avg_amt\", \"std_amt\", \"max_amt\", \"min_amt\",\n",
    "    \"avg_gap\", \"std_gap\", \"max_gap\", \"min_gap\"\n",
    "]\n",
    "X = agg_pd[feature_cols].values\n",
    "y = agg_pd[\"label\"].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "agg_pd[\"cv_fold\"] = -1\n",
    "cv_pred = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"  ğŸš© Fold {fold}/10 è¨“ç·´èˆ‡æ¸¬è©¦ ...\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf = SklearnLR(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    precs.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recs.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "    f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "    agg_pd.loc[test_idx, \"cv_fold\"] = fold\n",
    "    for idx, p in zip(test_idx, y_pred):\n",
    "        cv_pred.append((agg_pd.iloc[idx][\"component\"], int(agg_pd.iloc[idx][\"label\"]), int(p), fold))\n",
    "t_lr_end = time.time()\n",
    "\n",
    "print(\"\\nğŸ“Œ TD-UF (GraphFrames) + LR (StratifiedKFold)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(accs)), \n",
    "    (\"Precision\", np.mean(precs)),\n",
    "    (\"Recall\", np.mean(recs)),\n",
    "    (\"F1 Score\", np.mean(f1s))\n",
    "], headers=[\"æŒ‡æ¨™\", \"å¹³å‡å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(f\"â±ï¸ Sklearn LR åæŠ˜è¨“ç·´+æ¨è«–é‹ç®—æ™‚é–“ï¼š{t_lr_end - t_lr_start:.2f} ç§’\")\n",
    "print(f\"â±ï¸ TD-UF åˆ†ç¾¤ + ç‰¹å¾µèšåˆ + Sklearn LR ç¸½é‹ç®—æ™‚é–“ï¼š{(t_tduf_end - t_tduf_start) + (t_lr_end - t_lr_start):.2f} ç§’\")\n",
    "\n",
    "# ====== æ¸…é™¤æš«å­˜æª”æ¡ˆ ======\n",
    "try:\n",
    "    os.remove(TEMP_CSV)\n",
    "    print(f\"ğŸ§¹ å·²è‡ªå‹•åˆªé™¤æš«å­˜ CSVï¼š{TEMP_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ åˆªé™¤æš«å­˜ CSV å¤±æ•—ï¼š{e}\")\n",
    "\n",
    "# ====== Step 7: Baselineï¼ˆå…¨æ¬„ä½ LR, ç”¨ PySparkï¼‰ ======\n",
    "print(\"ğŸš© Baseline å…¨ç‰¹å¾µé‚è¼¯è¿´æ­¸è¨“ç·´ä¸­ ...\")\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df) for c in categorical_cols]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "feature_cols_full = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_full, outputCol=\"features\")\n",
    "vector_df = assembler.transform(df.withColumn(\"label\", col(\"Is_laundering\"))).select(\"features\", \"label\")\n",
    "train_b, val_b, test_b = vector_df.randomSplit([0.7, 0.2, 0.1], seed=42)\n",
    "\n",
    "lr = SparkLR(maxIter=100, regParam=0.01)\n",
    "t2 = time.time()\n",
    "model_base = lr.fit(train_b)\n",
    "result_base = model_base.transform(test_b)\n",
    "t3 = time.time()\n",
    "print(f\"â±ï¸ Baseline LR (PySpark) è¨“ç·´èˆ‡æ¨è«–é‹ç®—æ™‚é–“: {t3 - t2:.2f} ç§’\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc_base = evaluator.setMetricName(\"accuracy\").evaluate(result_base)\n",
    "f1_base  = evaluator.setMetricName(\"f1\").evaluate(result_base)\n",
    "prec_base = evaluator.setMetricName(\"weightedPrecision\").evaluate(result_base)\n",
    "rec_base = evaluator.setMetricName(\"weightedRecall\").evaluate(result_base)\n",
    "\n",
    "print(\"\\nğŸ“Œ Baseline LR (PySpark)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", acc_base), \n",
    "    (\"Precision\", prec_base), \n",
    "    (\"Recall\", rec_base), \n",
    "    (\"F1 Score\", f1_base), \n",
    "    (\"Time\", t3 - t2)\n",
    "], headers=[\"æŒ‡æ¨™\", \"æ•¸å€¼\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== Step 8: ç²¾æº–åº¦æ¯”è¼ƒåœ–è¡¨ ======\n",
    "print(\"ğŸ“Š ç•«å‡º TD-UF èˆ‡ Baseline ç²¾æº–åº¦æ¯”è¼ƒåœ– ...\")\n",
    "labels = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x - 0.2, [np.mean(precs), np.mean(recs), np.mean(f1s)], width=0.4, label=\"TD-UF + LR\")\n",
    "plt.bar(x + 0.2, [prec_base, rec_base, f1_base], width=0.4, label=\"Baseline LR\")\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"TD-UF (GraphFrames) vs Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"âœ… å…¨éƒ¨æµç¨‹åŸ·è¡Œå®Œç•¢ï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
