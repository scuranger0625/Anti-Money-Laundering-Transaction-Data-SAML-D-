{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400163af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動 SparkSession...\n",
      "✔️ SparkSession 建立完成\n",
      "📥 讀取資料中 ...\n",
      "✔️ 載入資料完成，共 9504852 筆交易\n",
      "🔄 產生 PoH 鏈式雜湊欄位 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 產生 PoH 雜湊鏈中...: 100%|██████████| 9504852/9504852 [00:17<00:00, 538917.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ PoH 雜湊計算完成\n",
      "🧩 TD-UF 步驟：建立交易圖的頂點與邊 ...\n",
      "✔️ 頂點數: 9504852，邊數: 0\n",
      "🔗 TD-UF 步驟：計算分群（connectedComponents） ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 完成分群計算\n",
      "🔗 TD-UF 步驟：Join 原始交易 ...\n",
      "✔️ Join 完成，資料筆數: 9504852\n",
      "📊 群組內聚合特徵中 ...\n",
      "✔️ 群組統計完成\n",
      "⏱️ TD-UF 分群 + 特徵聚合運算時間：92.99 秒\n",
      "🚚 一次 collect 到 pandas ...（用完自動刪）\n",
      "⏱️ toPandas() 運算時間：147.96 秒\n",
      "✔️ 暫存檔案儲存完成 (tduf_group_features_temp.csv) 共 9504852 筆\n",
      "🔄 十折分層交叉驗證中 ...\n",
      "  🚩 Fold 1/10 訓練與測試 ...\n",
      "  🚩 Fold 2/10 訓練與測試 ...\n",
      "  🚩 Fold 3/10 訓練與測試 ...\n",
      "  🚩 Fold 4/10 訓練與測試 ...\n",
      "  🚩 Fold 5/10 訓練與測試 ...\n",
      "  🚩 Fold 6/10 訓練與測試 ...\n",
      "  🚩 Fold 7/10 訓練與測試 ...\n",
      "  🚩 Fold 8/10 訓練與測試 ...\n",
      "  🚩 Fold 9/10 訓練與測試 ...\n",
      "  🚩 Fold 10/10 訓練與測試 ...\n",
      "\n",
      "📌 TD-UF (GraphFrames) + LR (StratifiedKFold)\n",
      "╒═══════════╤════════════╕\n",
      "│ 指標      │     平均值 │\n",
      "╞═══════════╪════════════╡\n",
      "│ Accuracy  │ 0.998966   │\n",
      "├───────────┼────────────┤\n",
      "│ Precision │ 1          │\n",
      "├───────────┼────────────┤\n",
      "│ Recall    │ 0.00465925 │\n",
      "├───────────┼────────────┤\n",
      "│ F1 Score  │ 0.00926792 │\n",
      "╘═══════════╧════════════╛\n",
      "⏱️ Sklearn LR 十折訓練+推論運算時間：884.13 秒\n",
      "⏱️ TD-UF 分群 + 特徵聚合 + Sklearn LR 總運算時間：977.12 秒\n",
      "🧹 已自動刪除暫存 CSV：tduf_group_features_temp.csv\n",
      "🚩 Baseline 全特徵邏輯迴歸訓練中 ...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o232.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚩 Baseline 全特徵邏輯迴歸訓練中 ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 196\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mc, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(df) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[0;32m    198\u001b[0m     df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚩 Baseline 全特徵邏輯迴歸訓練中 ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived_currency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceiver_bank_location\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 196\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [\u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[0;32m    198\u001b[0m     df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o232.fit.\n: org.apache.spark.SparkException: Input column Payment_currency does not exist.\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat_ws, monotonically_increasing_id, lag, collect_list\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg, stddev, count\n",
    "from graphframes import GraphFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLR\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 強制設定 Java/Hadoop 路徑\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Gephi-0.10.1\\jre-x64\\jdk-11.0.17+8-jre\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Winutils\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\";\" + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# ====== Step 0: 建立 SparkSession ======\n",
    "print(\"🚀 啟動 SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TD-UF (GraphFrames) vs Logistic Regression\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"file:///C:/Users/Leon/Desktop/spark-checkpoint\")\n",
    "print(\"✔️ SparkSession 建立完成\")\n",
    "\n",
    "# ====== Step 1: 日期字串轉 epoch timestamp ======\n",
    "def to_epoch(dt):\n",
    "    try:\n",
    "        return int(time.mktime(time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    except Exception:\n",
    "        return 0\n",
    "udf_epoch = udf(to_epoch, IntegerType())\n",
    "\n",
    "# ====== Step 2: 讀取資料，加 tx_id、timestamp ======\n",
    "print(\"📥 讀取資料中 ...\")\n",
    "raw_path = r\"C:\\Users\\Leon\\Desktop\\程式語言資料\\python\\TD-UF\\Anti Money Laundering Transaction Data (SAML-D)\\SAML-D.csv\"\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"timestamp\", udf_epoch(concat_ws(\" \", col(\"Date\").cast(\"string\"), col(\"Time\").cast(\"string\"))))\n",
    "df = df.withColumn(\"tx_id\", monotonically_increasing_id())\n",
    "print(f\"✔️ 載入資料完成，共 {df.count()} 筆交易\")\n",
    "\n",
    "# ====== Step 3: 計算 PoH 鏈式雜湊 ======\n",
    "def poh_chain(records):\n",
    "    pohs = []\n",
    "    prev_poh = \"\"\n",
    "    for row in tqdm(records, desc=\"🔗 產生 PoH 雜湊鏈中...\"):\n",
    "        txid_str = str(row['tx_id'])\n",
    "        poh = hashlib.sha256((prev_poh + txid_str).encode()).hexdigest()\n",
    "        pohs.append(poh)\n",
    "        prev_poh = poh\n",
    "    return pohs\n",
    "\n",
    "print(\"🔄 產生 PoH 鏈式雜湊欄位 ...\")\n",
    "tx_pd = df.orderBy(\"tx_id\").select(\n",
    "    \"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\"\n",
    ").toPandas()\n",
    "tx_pd[\"poh\"] = poh_chain(tx_pd.to_dict('records'))\n",
    "df = spark.createDataFrame(tx_pd)\n",
    "print(\"✔️ PoH 雜湊計算完成\")\n",
    "\n",
    "tx = df.select(\"tx_id\", \"Sender_account\", \"Receiver_account\", \"timestamp\", \"Amount\", \"Is_laundering\", \"poh\")\n",
    "\n",
    "# ====== Step 3.5: TD-UF 分群 + 聚合總運算時間（記錄 runtime） ======\n",
    "t_tduf_start = time.time()\n",
    "\n",
    "# ====== TD-UF 分群 ======\n",
    "print(\"🧩 TD-UF 步驟：建立交易圖的頂點與邊 ...\")\n",
    "vertices = tx.select(col(\"tx_id\").alias(\"id\")).distinct()\n",
    "edges_df = (\n",
    "    tx.alias(\"a\").join(tx.alias(\"b\"),\n",
    "    (col(\"a.Receiver_account\") == col(\"b.Sender_account\")) &\n",
    "    (col(\"a.timestamp\") < col(\"b.timestamp\")),\n",
    "    \"inner\")\n",
    "    .select(col(\"a.tx_id\").alias(\"src\"), col(\"b.tx_id\").alias(\"dst\"))\n",
    "    .distinct()\n",
    ")\n",
    "print(f\"✔️ 頂點數: {vertices.count()}，邊數: {edges_df.count()}\")\n",
    "\n",
    "print(\"🔗 TD-UF 步驟：計算分群（connectedComponents） ...\")\n",
    "gf = GraphFrame(vertices, edges_df)\n",
    "components = gf.connectedComponents()\n",
    "print(f\"✔️ 完成分群計算\")\n",
    "\n",
    "print(\"🔗 TD-UF 步驟：Join 原始交易 ...\")\n",
    "dfc = components.join(tx, components.id == tx.tx_id)\n",
    "print(f\"✔️ Join 完成，資料筆數: {dfc.count()}\")\n",
    "\n",
    "# ====== Step 4: 聚合特徵 ======\n",
    "print(\"📊 群組內聚合特徵中 ...\")\n",
    "w = Window.partitionBy(\"component\").orderBy(\"timestamp\")\n",
    "dfc = (dfc.withColumn(\"prev_ts\", lag(\"timestamp\", 1).over(w))\n",
    "           .withColumn(\"gap\", col(\"timestamp\") - col(\"prev_ts\")))\n",
    "\n",
    "agg_df = (\n",
    "    dfc.groupBy(\"component\")\n",
    "    .agg(\n",
    "        spark_min(\"timestamp\").alias(\"start_ts\"),\n",
    "        count(\"timestamp\").alias(\"tx_count\"),\n",
    "        avg(\"Amount\").alias(\"avg_amt\"),\n",
    "        stddev(\"Amount\").alias(\"std_amt\"),\n",
    "        spark_max(\"Amount\").alias(\"max_amt\"),\n",
    "        spark_min(\"Amount\").alias(\"min_amt\"),\n",
    "        avg(\"gap\").alias(\"avg_gap\"),\n",
    "        stddev(\"gap\").alias(\"std_gap\"),\n",
    "        spark_max(\"gap\").alias(\"max_gap\"),\n",
    "        spark_min(\"gap\").alias(\"min_gap\"),\n",
    "        spark_max(\"Is_laundering\").alias(\"label\"),\n",
    "        collect_list(\"poh\").alias(\"poh_hashes\")\n",
    "    ).fillna({\"std_amt\": 0, \"std_gap\": 0})\n",
    ")\n",
    "print(\"✔️ 群組統計完成\")\n",
    "t_tduf_end = time.time()\n",
    "print(f\"⏱️ TD-UF 分群 + 特徵聚合運算時間：{t_tduf_end - t_tduf_start:.2f} 秒\")   # <--- 計時\n",
    "\n",
    "# ====== Step 5: 一次 collect ======\n",
    "print(\"🚚 一次 collect 到 pandas ...（用完自動刪）\")\n",
    "t_collect_start = time.time()\n",
    "agg_pd = agg_df.toPandas()\n",
    "t_collect_end = time.time()\n",
    "print(f\"⏱️ toPandas() 運算時間：{t_collect_end - t_collect_start:.2f} 秒\")\n",
    "\n",
    "# ==== 儲存暫存檔案 ====\n",
    "TEMP_CSV = \"tduf_group_features_temp.csv\"\n",
    "agg_pd[\"poh_hashes\"] = agg_pd[\"poh_hashes\"].apply(lambda x: \";\".join(map(str, x)))\n",
    "agg_pd.to_csv(TEMP_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✔️ 暫存檔案儲存完成 ({TEMP_CSV}) 共 {len(agg_pd)} 筆\")\n",
    "\n",
    "# ====== Step 6: 讀取暫存檔案，做十折交叉驗證與邏輯迴歸計時 ======\n",
    "print(\"🔄 十折分層交叉驗證中 ...\")\n",
    "t_lr_start = time.time()\n",
    "agg_pd = pd.read_csv(TEMP_CSV)\n",
    "agg_pd = agg_pd.fillna(0)\n",
    "feature_cols = [\n",
    "    \"tx_count\", \"avg_amt\", \"std_amt\", \"max_amt\", \"min_amt\",\n",
    "    \"avg_gap\", \"std_gap\", \"max_gap\", \"min_gap\"\n",
    "]\n",
    "X = agg_pd[feature_cols].values\n",
    "y = agg_pd[\"label\"].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "agg_pd[\"cv_fold\"] = -1\n",
    "cv_pred = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"  🚩 Fold {fold}/10 訓練與測試 ...\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf = SklearnLR(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    precs.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recs.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "    f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "    agg_pd.loc[test_idx, \"cv_fold\"] = fold\n",
    "    for idx, p in zip(test_idx, y_pred):\n",
    "        cv_pred.append((agg_pd.iloc[idx][\"component\"], int(agg_pd.iloc[idx][\"label\"]), int(p), fold))\n",
    "t_lr_end = time.time()\n",
    "\n",
    "print(\"\\n📌 TD-UF (GraphFrames) + LR (StratifiedKFold)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", np.mean(accs)), \n",
    "    (\"Precision\", np.mean(precs)),\n",
    "    (\"Recall\", np.mean(recs)),\n",
    "    (\"F1 Score\", np.mean(f1s))\n",
    "], headers=[\"指標\", \"平均值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(f\"⏱️ Sklearn LR 十折訓練+推論運算時間：{t_lr_end - t_lr_start:.2f} 秒\")\n",
    "print(f\"⏱️ TD-UF 分群 + 特徵聚合 + Sklearn LR 總運算時間：{(t_tduf_end - t_tduf_start) + (t_lr_end - t_lr_start):.2f} 秒\")\n",
    "\n",
    "# ====== 清除暫存檔案 ======\n",
    "try:\n",
    "    os.remove(TEMP_CSV)\n",
    "    print(f\"🧹 已自動刪除暫存 CSV：{TEMP_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 刪除暫存 CSV 失敗：{e}\")\n",
    "\n",
    "# ====== Step 7: Baseline（全欄位 LR, 用 PySpark） ======\n",
    "print(\"🚩 Baseline 全特徵邏輯迴歸訓練中 ...\")\n",
    "categorical_cols = [\"Payment_currency\", \"Received_currency\", \"Sender_bank_location\", \"Receiver_bank_location\", \"Payment_type\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\").fit(df) for c in categorical_cols]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "feature_cols_full = [\"Amount\", \"timestamp\"] + [f\"{c}_idx\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_full, outputCol=\"features\")\n",
    "vector_df = assembler.transform(df.withColumn(\"label\", col(\"Is_laundering\"))).select(\"features\", \"label\")\n",
    "train_b, val_b, test_b = vector_df.randomSplit([0.7, 0.2, 0.1], seed=42)\n",
    "\n",
    "lr = SparkLR(maxIter=100, regParam=0.01)\n",
    "t2 = time.time()\n",
    "model_base = lr.fit(train_b)\n",
    "result_base = model_base.transform(test_b)\n",
    "t3 = time.time()\n",
    "print(f\"⏱️ Baseline LR (PySpark) 訓練與推論運算時間: {t3 - t2:.2f} 秒\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc_base = evaluator.setMetricName(\"accuracy\").evaluate(result_base)\n",
    "f1_base  = evaluator.setMetricName(\"f1\").evaluate(result_base)\n",
    "prec_base = evaluator.setMetricName(\"weightedPrecision\").evaluate(result_base)\n",
    "rec_base = evaluator.setMetricName(\"weightedRecall\").evaluate(result_base)\n",
    "\n",
    "print(\"\\n📌 Baseline LR (PySpark)\")\n",
    "print(tabulate([\n",
    "    (\"Accuracy\", acc_base), \n",
    "    (\"Precision\", prec_base), \n",
    "    (\"Recall\", rec_base), \n",
    "    (\"F1 Score\", f1_base), \n",
    "    (\"Time\", t3 - t2)\n",
    "], headers=[\"指標\", \"數值\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# ====== Step 8: 精準度比較圖表 ======\n",
    "print(\"📊 畫出 TD-UF 與 Baseline 精準度比較圖 ...\")\n",
    "labels = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x - 0.2, [np.mean(precs), np.mean(recs), np.mean(f1s)], width=0.4, label=\"TD-UF + LR\")\n",
    "plt.bar(x + 0.2, [prec_base, rec_base, f1_base], width=0.4, label=\"Baseline LR\")\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"TD-UF (GraphFrames) vs Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"✅ 全部流程執行完畢！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
